# ğŸŒ CSSE-415 Team 5 â€“ Predicting the Gini Index ğŸ¯

## ğŸ“– Full Project Summary
In this project, we developed a comprehensive, end-to-end pipeline to forecast next-year Gini coefficientsâ€”the most widely used measure of income inequalityâ€”for 163 countries using World Bank Indicators (2002â€“2020). We began by ingesting and merging raw CSVs, then rigorously cleaned and imputed missing values (per-country means with global fallback), producing a complete dataset of socio-economic metrics. We engineered three complementary feature representations:  
- **X1:** single-year lags  
- **X3:** multi-year (1â€“5-year) lags  
- **X2:** annual value plus 5-year rolling mean & standard deviation  

Next, we benchmarked a diverse suite of modelsâ€”from a naive bias baseline, through linear regressions (with and without forward feature selection), k-nearest neighbors, and random forests, to gradient boosted treesâ€”using group-aware cross-validation to prevent country-level leakage. Our top performer, **Gradient Boosted Trees on rolling statistics (X2)**, achieved **53.6 % RÂ²**, and surfaced the key drivers of global inequality:  
- ğŸ’¥ **Homicide rate** (per 100 000 people)  
- ğŸŒ² **Forest land** (% of land area)  
- âš¡ **Access to electricity** (% of population)  
- âš°ï¸ **Death rate** (per 1 000 people)  

Finally, we serialized our best model (`best_gbr_model.pkl`) and scaler (`scaler.pkl`) and wrapped the workflow into an **interactive Jupyter demo**, allowing users to select any country & year to generate on-demand Gini forecasts. This framework not only delivers strong predictive performance but also empowers policymakers and researchers with actionable insights into the socioeconomic factors driving inequality.

## ğŸ§‘â€ğŸ’» Authors
- Abdullah Islam  
- Agnay Srivastava  
- Parth Sundaram  
- Steven Johnson  

---
## ğŸ“‚ Directory Structure

```text
.
â”œâ”€â”€ World Bank Indicators.pdf         ğŸ“‘  (project report & slides)
â””â”€â”€ Code/
    â”œâ”€â”€ 01_Clean_Impute.ipynb         ğŸ§¼  Data cleaning & imputation
    â”œâ”€â”€ 02_Simple_Regressors.ipynb    ğŸ”¢  Benchmark basic regressors
    â”œâ”€â”€ 02b_LR_with_FS.ipynb          ğŸ¯  Linear regression + feature selection
    â”œâ”€â”€ 03_RandomForest.ipynb         ğŸŒ³  Random Forest hyperparameter tuning
    â”œâ”€â”€ 04_Demo_Prep.ipynb            ğŸ› ï¸  Demo data preparation
    â”œâ”€â”€ 05_Demo.ipynb                 ğŸš€  Interactive prediction demo
    â”œâ”€â”€ best_gbr_model.pkl            ğŸ“¦  Serialized best GBT model
    â”œâ”€â”€ scaler.pkl                    âš–ï¸  Feature scaler for demo
    â”œâ”€â”€ world_bank.csv                ğŸŒ  Raw merged data
    â”œâ”€â”€ groups_train.csv              ğŸš‰  Training group assignments
    â”œâ”€â”€ groups_test.csv               ğŸš¥  Test group assignments
    â”œâ”€â”€ groups_demo.csv               ğŸ¬  Demo group assignments
    â”œâ”€â”€ X1_train.csv                  â³  1-year lag features (train)
    â”œâ”€â”€ X1_test.csv                   â³  1-year lag features (test)
    â”œâ”€â”€ X1_demo.csv                   â³  1-year lag features (demo)
    â”œâ”€â”€ X2_train.csv                  ğŸ“ˆ  Year + 5-yr rolling stats (train)
    â”œâ”€â”€ X2_test.csv                   ğŸ“ˆ  Year + 5-yr rolling stats (test)
    â”œâ”€â”€ X2_demo.csv                   ğŸ“ˆ  Year + 5-yr rolling stats (demo)
    â”œâ”€â”€ X3_train.csv                  ğŸ”„  1â€“5-yr lag features (train)
    â”œâ”€â”€ X3_test.csv                   ğŸ”„  1â€“5-yr lag features (test)
    â”œâ”€â”€ X3_demo.csv                   ğŸ”„  1â€“5-yr lag features (demo)
    â”œâ”€â”€ y_train.csv                   ğŸ¯  Target Gini (train)
    â”œâ”€â”€ y_test.csv                    ğŸ¯  Target Gini (test)
    â”œâ”€â”€ y_demo.csv                    ğŸ¯  Target Gini (demo)
    â”œâ”€â”€ Data Dump/                    ğŸ“‚  Raw & imputed CSVs by method
    â”‚   â”œâ”€â”€ world_bank.csv           ğŸŒ  Raw data copy
    â”‚   â”œâ”€â”€ cleaned.csv              ğŸ§¹  Cleaned & default-imputed dataset
    â”‚   â”œâ”€â”€ Mean/                    â–  Mean imputation
    â”‚   â”‚   â”œâ”€â”€ mean_train.csv       â–  Train set
    â”‚   â”‚   â”œâ”€â”€ mean_test.csv        â–  Test set
    â”‚   â”‚   â””â”€â”€ cleaned_mean.csv     â–  Full dataset
    â”‚   â”œâ”€â”€ KNN/                     ğŸ¤  KNN imputation
    â”‚   â”‚   â”œâ”€â”€ knn_train.csv        ğŸ¤  Train set
    â”‚   â”‚   â”œâ”€â”€ knn_test.csv         ğŸ¤  Test set
    â”‚   â”‚   â””â”€â”€ cleaned_knn.csv      ğŸ¤  Full dataset
    â”‚   â”œâ”€â”€ MICE/                    ğŸ§©  MICE imputation
    â”‚   â”‚   â”œâ”€â”€ mice_train.csv       ğŸ§©  Train set
    â”‚   â”‚   â”œâ”€â”€ mice_test.csv        ğŸ§©  Test set
    â”‚   â”‚   â””â”€â”€ cleaned_mice.csv     ğŸ§©  Full dataset
    â”‚   â””â”€â”€ Forest/                  ğŸŒ²  MissForest imputation
    â”‚       â”œâ”€â”€ forest_train.csv     ğŸŒ²  Train set
    â”‚       â”œâ”€â”€ forest_test.csv      ğŸŒ²  Test set
    â”‚       â””â”€â”€ cleaned_forest.csv   ğŸŒ²  Full dataset
    â””â”€â”€ Old Notebooks/               ğŸ“š  Archived exploratory work
        â”œâ”€â”€ 00_Preliminary_Analysis.ipynb   ğŸ“š Initial exploration
        â”œâ”€â”€ 01_Clean_Impute-Copy1.ipynb     ğŸ“š Early imputation trials
        â”œâ”€â”€ 02_Simple_Regressors-old.ipynb  ğŸ“š Legacy baselines
        â””â”€â”€ Prelimary_Work.ipynb            ğŸ“š Misc. preliminary analyses

```


## ğŸ”§ Dependencies & Setup
- **Python 3.8+**, **Jupyter Notebook**  
- **pandas**, **numpy**, **scikit-learn**, **joblib**, **matplotlib**, **xgboost** (or `sklearn.ensemble.GradientBoostingRegressor`)  
- Install essentials with:
  ```bash
  pip install pandas numpy scikit-learn matplotlib joblib xgboost
## ğŸš€ Usage Workflow

1. **Data Cleaning & Imputation**  
   - **Notebook:** `01_Clean_Impute.ipynb`  
   - **Steps:**  
     - Load raw `world_bank.csv` (2002â€“2020)  
     - Drop countries & indicators with excessive missingness  
     - Impute missing entries (per-country mean â†’ global fallback)  
     - Split into feature sets & target:  
       - **X1:** 1-year lag (`X1_train.csv`, `X1_test.csv`, `X1_demo.csv`)  
       - **X2:** year + 5-yr rolling mean & std (`X2_*.csv`)  
       - **X3:** 1â€“5-yr lags (`X3_*.csv`)  
       - **y:** next-year Gini (`y_*.csv`)

2. **Baseline Modeling**  
   - **Notebook:** `02_Simple_Regressors.ipynb`  
   - **Models:**  
     - ğŸ¯ Bias-only baseline  
     - ğŸ“ Linear Regression  
     - ğŸ“ Ridge (Î± grid)  
     - ğŸ”— Lasso (Î± grid)  
     - ğŸ¤ K-Nearest Neighbors  
     - ğŸŒ³ Random Forest  
     - ğŸš€ Gradient Boosted Trees  
   - **Evaluation:** Group-aware cross-validation â†’ RÂ² scores for Sets 1, 2, 3.

3. **Feature Selection**  
   - **Notebook:** `02b_LR_with_FS.ipynb`  
   - **Method:** Forward stepwise selection on linear model  
   - **Outcome:** Identify top predictors, compare against full-feature model.

4. **Random Forest Tuning**  
   - **Notebook:** `03_RandomForest.ipynb`  
   - **Hyperparameters:**  
     - `n_estimators` âˆˆ [50, 260]  
     - `max_depth` âˆˆ [5, 20]  
   - **Goal:** Optimize RF per feature set for best RÂ².

5. **Demo Preparation**  
   - **Notebook:** `04_Demo_Prep.ipynb`  
   - **Tasks:**  
     - Load `best_gbr_model.pkl` & `scaler.pkl`  
     - Assemble feature vector for chosen country & year  

6. **Interactive Demo**  
   - **Notebook:** `05_Demo.ipynb`  
   - **Flow:**  
     1. Select country from `groups_demo.csv`  
     2. Notebook scales inputs & predicts next-year Gini  
     3. Display result & featureâ€importance summary  

---

## ğŸ“Š Results Summary

| Model                          | Set 1 (1-yr) | Set 2 (rolling) | Set 3 (lags) | **Avg RÂ² %** |
|:-------------------------------|-------------:|---------------:|------------:|-------------:|
| **Bias-only**                  |        â€“2.2  |          â€“2.2  |      â€“2.2  |       â€“2.2   |
| **Linear Regression**          |        39.9  |          53.4  |      43.0  |       45.1   |
| **Linear + FS**                |        40.0  |          53.1  |      44.2  |       45.8   |
| **Ridge (Î±=100â€“1000)**         |        34.7  |          42.5  |      38.4  |       38.5   |
| **Lasso (Î±=0.01â€“1)**           |        35.6  |          51.1  |      33.3  |       40.0   |
| **Random Forest (50â€“260)**     |        54.3  |          49.3  |      49.7  |       51.1   |
| **Gradient Boosted Trees**     |        50.6  |          53.6  |      50.4  |       51.5   |
| **K-Nearest Neighbors (kâ‰ˆ40)** |        45.4  |          43.9  |      45.4  |       44.6   |

> ğŸš€ **Top Performer:** Gradient Boosted Trees on Rolling Stats (Set 2) with **53.6 % RÂ²**

---

## ğŸŒŸ Key Drivers (GBT on Set 2)

1. ğŸ’¥ **Intentional Homicides** (per 100 000 people)  
2. ğŸŒ² **Forest Land** (% of land area)  
3. âš¡ **Electricity Access** (% of population)  
4. âš°ï¸ **Death Rate** (per 1 000 people)  

---

## ğŸ’¡ Next Steps & Extensions

- ğŸ”¬ **Advanced Imputation**  
  - Compare MICE & MissForest in-depth  
- ğŸ“ **Feature Engineering**  
  - Create ratio features (e.g., GDP / capita, health expenditure / population)  
- ğŸ¯ **Dimensionality Reduction**  
  - Apply PCA or autoencoders to denoise input space  
- âš–ï¸ **Fairness Analysis**  
  - Evaluate bias across income tiers & regions  
- ğŸ¤– **Bayesian & Hierarchical Models**  
  - Quantify uncertainty, incorporate multi-level priors  
- ğŸ“ˆ **Ensemble Blends**  
  - Combine top models from Sets 1â€“3 for robust forecasts  
